{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    precision_recall_curve, auc, precision_score, recall_score, f1_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb20848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"C:/Users/maxhi/Documents/GitHub/Credit_Card_Fraud_Detection/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data check\n",
    "print(df.shape)\n",
    "print(df['Class'].value_counts(normalize=True))  # Shows class imbalance - fraud rate of ~0.17%\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67748ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale 'Amount' and 'Time' only, as v1-v28 are already standardised (via PCA)\n",
    "scaler = StandardScaler()\n",
    "df[['Scaled_Amount', 'Scaled_Time']] = scaler.fit_transform(df[['Amount', 'Time']])\n",
    "df.drop(['Amount', 'Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6245608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (stratified), 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle imbalance with SMOTE -- decided on SMOTE, as planning to use log regression, and this will help with underfitting\n",
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "smote = SMOTE(random_state=10)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE:\", y_train_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression with class weighting -- decided on log regression as it's a great baseline and should handle the imbalance well. Also, as features are PCA-transformed,\n",
    "# data may be linearly seperable, also beneficial to log regression\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=10)\n",
    "model.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0296d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(f\"\\nROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "labels = np.array([[f\"{v}\\n{p:.1f}%\" for v, p in zip(row_raw, row_pct)]\n",
    "                   for row_raw, row_pct in zip(cm, cm_percent)])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_percent, annot=labels, fmt='', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Pred 0', 'Pred 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title(\"Confusion Matrix (Counts + %)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe6340",
   "metadata": {},
   "source": [
    "As we can see, this is quite a recall-biased model - perfect for situations where failing to capture fraud is costly, and false positives are not costly to follow up. Let's see if some model tuning can improve the precision, without sacrificing too much recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c652fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold tuning\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (y_proba > t).astype(int)\n",
    "    precision_scores.append(precision_score(y_test, y_pred_thresh, zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_thresh))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "# Plot precision-recall-f1 vs. threshold\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, precision_scores, label=\"Precision\")\n",
    "plt.plot(thresholds, recall_scores, label=\"Recall\")\n",
    "plt.plot(thresholds, f1_scores, label=\"F1 Score\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Threshold Tuning\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select threshold that maximizes F1 score\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"\\nBest Threshold (F1): {best_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce68f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation using selected threshold\n",
    "y_pred_final = (y_proba > best_threshold).astype(int)\n",
    "\n",
    "# ROC and PR AUC\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(f\"\\nROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "labels = np.array([[f\"{v}\\n{p:.1f}%\" for v, p in zip(row_raw, row_pct)]\n",
    "                   for row_raw, row_pct in zip(cm, cm_percent)])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_percent, annot=labels, fmt='', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Pred 0', 'Pred 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title(\"Confusion Matrix (Counts + %)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aaf1e6",
   "metadata": {},
   "source": [
    "After threshold tuning, we've significantly improved the model's precision — meaning fewer false alarms — while still capturing the majority of fraudulent cases. This version strikes a more balanced trade-off between recall and precision, making it better suited for environments where follow-up costs or investigation fatigue are non-trivial, but missing fraud still carries significant risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation Check (StratifiedKFold)\n",
    "# To ensure the model's performance wasn't overly reliant on a single train-test split, run 5-fold stratified cross-validation using SMOTE within each fold.\n",
    "\n",
    "# Define model pipeline with SMOTE inside (so each fold applies resampling correctly)\n",
    "\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    ('smote', SMOTE(random_state=10)),\n",
    "    ('model', LogisticRegression(solver='liblinear', class_weight='balanced', random_state=10))\n",
    "])\n",
    "\n",
    "# Custom scorer for PR AUC\n",
    "def pr_auc_score(y_true, y_probs):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "# Define scorers\n",
    "scoring = {\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True),\n",
    "    'pr_auc': make_scorer(pr_auc_score, needs_proba=True)\n",
    "}\n",
    "\n",
    "# Perform Stratified 5-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = cross_validate(pipeline, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nCross-Validated ROC AUC: {:.4f} ± {:.4f}\".format(cv_results['test_roc_auc'].mean(), cv_results['test_roc_auc'].std()))\n",
    "print(\"Cross-Validated PR AUC: {:.4f} ± {:.4f}\".format(cv_results['test_pr_auc'].mean(), cv_results['test_pr_auc'].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf92382",
   "metadata": {},
   "source": [
    "These scores are consistent with our earlier evaluation, and the low standard deviations suggest the model performs reliably across different data splits. The strong ROC AUC indicates excellent overall discrimination between fraud and non-fraud, while the PR AUC — more sensitive to class imbalance — confirms the model maintains good precision-recall balance in a realistic, imbalanced setting.\n",
    "\n",
    "This further supports the model’s generalisability and suggests it would likely perform well on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c86f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explanation\n",
    "explainer = shap.LinearExplainer(model, X_train_resampled, feature_perturbation=\"interventional\")\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize feature importance\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b06666",
   "metadata": {},
   "source": [
    "The SHAP summary plot shows that the model is driven primarily by a small set of features, with V14, V17, and V12 having the highest average impact on predictions.\n",
    "\n",
    "Supporting features like V1, V16, and V7 also contribute meaningfully, while Scaled_Amount provides some signal but is not a major driver.\n",
    "\n",
    "Overall, the model appears to focus on a handful of key fraud indicators, which supports the idea that it’s learning strong, generalisable patterns rather than overfitting to noise."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
