{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    precision_recall_curve, auc, precision_score, recall_score, f1_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb20848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"C:/Users/maxhi/Documents/GitHub/Credit_Card_Fraud_Detection/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data check\n",
    "print(df.shape)\n",
    "print(df['Class'].value_counts(normalize=True))  # Shows class imbalance - fraud rate of ~0.17%\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67748ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale 'Amount' and 'Time' only, as v1-v28 are already standardised (via PCA)\n",
    "scaler = StandardScaler()\n",
    "df[['Scaled_Amount', 'Scaled_Time']] = scaler.fit_transform(df[['Amount', 'Time']])\n",
    "df.drop(['Amount', 'Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6245608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc1882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (stratified), 30% test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle imbalance with SMOTE -- decided on SMOTE, as planning to use log regression, and this will help with underfitting\n",
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "smote = SMOTE(random_state=10)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE:\", y_train_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5d2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression with class weighting -- decided on log regression as it's a great baseline and should handle the imbalance well. Also, as features are PCA-transformed,\n",
    "# data may be linearly seperable, also beneficial to log regression\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=10)\n",
    "model.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0296d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(f\"\\nROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "labels = np.array([[f\"{v}\\n{p:.1f}%\" for v, p in zip(row_raw, row_pct)]\n",
    "                   for row_raw, row_pct in zip(cm, cm_percent)])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_percent, annot=labels, fmt='', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Pred 0', 'Pred 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.title(\"Confusion Matrix (Counts + %)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe6340",
   "metadata": {},
   "source": [
    "As we can see, this is quite a recall-biased model - perfect for situations where failing to capture fraud is costly, and false positives are not costly to follow up. Let's see if some model tuning can improve the precision, without sacrificing too much recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c652fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold tuning\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_thresh = (y_proba > t).astype(int)\n",
    "    precision_scores.append(precision_score(y_test, y_pred_thresh, zero_division=0))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_thresh))\n",
    "    f1_scores.append(f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "# Plot precision-recall-f1 vs. threshold\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, precision_scores, label=\"Precision\")\n",
    "plt.plot(thresholds, recall_scores, label=\"Recall\")\n",
    "plt.plot(thresholds, f1_scores, label=\"F1 Score\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Threshold Tuning\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select threshold that maximizes F1 score\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"\\nBest Threshold (F1): {best_threshold:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
